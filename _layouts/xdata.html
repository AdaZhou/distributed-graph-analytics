<html>
{% include head.html %}
<body>
{% include navbar.html %}
<div class="container-fluid">
    <h3><a name="real-data-examples" class="anchor" href="#real-data-examples"><span class="glyphicon glyphicon-link"></span></a> Walkthrough With Data</h3>
    <h4><a name="prereqs" class="anchor" href="#prereqs"><span class="glyphicon glyphicon-link"></span></a> Prerequisites</h4>
    <ol>
        <li>
            <a href="/distributed-graph-analytics/HowToGet/">How To Get DGA</a>
        </li>
        <li>
            <a href="/distributed-graph-analytics/HowToBuild/">How To Build DGA</a>
        </li>
        <li>
            <a href="/distributed-graph-analytics/HowToDeploy/">How To Deploy DGA</a>
        </li>
    </ol>
    <h3><a name="started" class="anchor" href="#started"><span class="glyphicon glyphicon-link"></span></a> Let's Get Started</h3>

    <p>In this example, we will be running <a href="/distributed-graph-analytics/{{page.analytic}}/">{{page.title}}</a> with DGA.</p>

    <p>First, let's get some sample data. Already have data you want to use? That's great! Make sure it follows <a
            href="/distributed-graph-analytics/FAQ/#how-data">this</a> format and it
        will work with DGA.</p>
<pre>
<code>
    $ wget http://sotera.github.io/distributed-graph-analytics/data/example.csv
</code>
</pre>
    <p>If everything checks out! We can now copy our data set to a directory in hdfs. For this example we will create a directory in tmp for the input. You don't need to use this directory all the time.</p>
<pre>
<code>
    $ hadoop fs -mkdir -p /tmp/dga/{{page.analytic}}/input/
</code>
</pre>
    <p>No need to create the output directory. That will be done for us when our job is complete.</p>

    <p>Now let's copy our data onto hdfs.</p>
<pre>
<code>
    $ hadoop fs -copyFromLocal example.csv /tmp/dga/{{page.analytic}}/input/
</code>
</pre>
    <p>Finally, we can now run our analytic! The command below uses the built in DGARunner to run {{page.title}}.</p>
<pre>
<code>
    $ cd /opt/dga/dist/
    $ hadoop jar dga-giraph-0.0.1.jar com.soteradefense.dga.DGARunner {{page.analytic}} /tmp/dga/{{page.analytic}}/input/ /tmp/dga/{{page.analytic}}/output/ -w 1 -ca io.edge.reverse.duplicator=true
</code>
</pre>
    <p>The command above, runs the dga-giraph-0.0.1.jar and executes the DGARunner class. It passes in 5 command line arguments.</p>
    <ul>
        <li>
            {{page.analytic}} - Tells the DGARunner which analytic it needs to run. This is required.
        </li>
        <li>
            /tmp/dga/{{page.analytic}}/input/ - Tells the DGARunner where the input data is located. This is required.
        </li>
        <li>
            /tmp/dga/{{page.analytic}}/output/ - Tells the DGARunner where to output the data. This is required.
        </li>
        <li>
            -w 1 - This gets passed to giraph. It tells giraph how many workers to use. This is optional.
        </li>
        <li>
            -ca io.edge.reverse.duplicator=true - This tells our input format to duplicate the edges so our graph becomes weakly connected. This is optional.
        </li>
    </ul>
    <p>Is it done yet? If so, lets see the results!</p>
<pre>
<code>
    $ cd
    $ mkdir results/
    $ cd results
    $ hadoop fs -copyToLocal /tmp/dga/{{page.analytic}}/output/* .
</code>
</pre>
    <p>What are all these parts? Don't worry, let's make them one!</p>
<pre>
<code>
    $ cat part-* >> bigfile.txt
    $ vi bigfile.txt
</code>
</pre>
    <p>And there you have it! You ran your first analytic with DGA!</p>
</div>
</body>
{% include footer.html %}
</html>